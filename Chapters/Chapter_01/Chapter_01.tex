\chapter{General concepts on supervised learning for industrial applications}
In the dynamic landscape of industrial applications, the integration of supervised learning techniques has emerged as a pivotal force in addressing complex challenges, particularly within the aerospace industry. This chapter delves into the foundational concepts of industrial-oriented supervised learning, focusing on its profound implications for solving regression problems inherent in various aerospace endeavors. From optimizing fuel consumption\cite{hong2018data} to enhancing structural integrity\cite{pfingstl2023warped}, numerous industrial dilemmas can be effectively framed as regression tasks, where supervised learning algorithms offer unparalleled potential for both solvability and computational efficiency. Within the realm of industry-oriented supervised learning, understanding the mathematical background of these methodologies lays the groundwork for harnessing their transformative impact on real-world industrial challenges, particularly within aerospace domains.\\
%
\indent Mathematically, the problems we refer to in this chapter consist of finding the explicit shape of a function
\begin{equation}\label{eq:F}
	\cal{F}:S_{\mathbf{X}}\rightarrow S_{\mathbf{Y}}
\end{equation}
with $S_{\mathbf{X}}\subset\mathbb{R}^n$ and $S_{\mathbf{Y}}\subset\mathbb{R}^m$; \ie the function $\cal{F}$ takes as input a vector $\mathbf{X}=\{x_1,x_2,\ldots,x_n\}$, and outputs a vector $\mathbf{Y}=\{y_1,y_2,\ldots,y_m\}$. The input vector could represent, for instance, a collection of loads applied to some part of the fuselage structure of an aeroplane, whereas the output vector may represent the probability of failure of that structure under the loads contained in $\mathbf{X}$. The problem at hand would therefore be finding the function $\cal{F}$ which maps load configurations to failure probabilities. By categorizing the task of finding such function $\cal{F}$ as a ''regression problem'', we mean usually $n>>m$. When speaking about regression problems, the input vector $\mathbf{X}$ is usually called the ''feature vector'', while the output vector $\mathbf{Y}$ is usually called --in a descriptive way-- the ''prediction vector''. Similarly, $n$ is called the ''feature'' or the ''input'' dimension, and $m$ is called the ''output'' or the ''prediction'' dimension. Note that finding the explicit shape of $\cal{F}$ might be a difficult task when $\cal{F}$ is a strongly non-linear function, especially if $n>>1$.\\
%
\indent Although regression problems can be tackled by several means, in this work we will focus on artificial neural networks or ANNs\cite{Marsland2015Machine}. From this approach, the starting point is a parametrized function with which we shall approximate $\mathcal{F}$, so that we have:

\begin{align}
	& \hat{\mathcal{F}} : S_{\mathbf{X}} \rightarrow S_{\mathbf{\hat{Y}}} \\
	& \hat{\mathcal{F}}(\mathbf{X}; W) = \mathbf{\hat{Y}} \label{eq:Fhat}
\end{align}

Where $W$ is the set of parameters defining the explicit shape of $\mathcal{\hat{F}}$. In this equation, the hat on top of $\mathcal{\hat{F}}$ and $\mathbf{\hat{Y}}$ accounts for the difference between our parametrized function and the real function we are trying to find, the one from \autoref{eq:F}. The approach now is to define a distance $\mathcal{L}=\|\mathcal{F}-\mathcal{\hat{F}}\|$ and treat the regression problem as the task of minimizing $\mathcal{L}$:
\begin{equation}\label{eq:Fargmin}
	\mathcal{\hat{F}}^*=argmin(\mathcal{L})
\end{equation}\\
%
\indent From a practical point of view, the information available for defining $\mathcal{L}$ consists of a (large\footnote{By large, we mean $\left|\text{Full}\right|>>n$ (cfr. \autoref{eq:full}).}) set of duplets $(\mathbf{X},\mathbf{Y})$\footnote{N.B. the lack of hat here, meaning data has been sampled from the domain and codomain of $\mathcal{F}$,$S_\mathbf{X}$ and $S_\mathbf{Y}$, respectively.} representing empirical samples from the feature and the prediction space coming from experimental data, simulations, or any other source. Such set is called Full:
\begin{equation}\label{eq:full}
	\text{Full}=\{\mathbf{X}_i,\mathbf{Y}_i\}_{i=1}^N
\end{equation}
The procedure is to split Full into a training and a test set, so that
\begin{align}
	& \mathcal{S}^\text{Train} \cup \mathcal{S}^\text{Test} = \text{Full} \\
	& \mathcal{S}^\text{Train} \cap \mathcal{S}^\text{Test} = \emptyset \\
	& |\mathcal{S}^\text{Train}| > |\mathcal{S}^\text{Test}|
\end{align}\\
%
$\mathcal{L}$ (which is called the ''loss function'') can now be defined as
\begin{align}
	& \mathcal{L}=\mathcal{L}(W) \\
	& \mathcal{L}(W)=E(W)+\lambda R(W) \label{eq:loss}
\end{align}
where
\begin{equation}\label{eq:error}
	E(W) = \|\mathcal{S}^\text{Train}_\mathbf{Y} - \mathcal{S}^\text{Train}_\mathbf{\hat{Y}}\|
\end{equation}
and
\begin{equation}\label{eq:regularization}
	R(W) = \|W\|
\end{equation}
for any defined norm $\|\cdot\|$. In \autoref{eq:loss}, $\lambda$ is just an arbitrary scaling factor. For the explicit dependence of $E$ on $W$, recall \autoref{eq:Fhat}\footnote{N.B. $S^\text{Test}_{\mathbf{\hat{Y}}}$ is not available beforehand; it is generated by applying $\mathcal{\hat{F}}_W$ to $S^\text{Test}_{\mathbf{X}}$.}.\\
%
\indent $\mathcal{L}$ is called the ''loss function''. We can see from \autoref{eq:loss} that it combines the effect of two terms: the error term (\autoref{eq:error}) accounts for the mismatch between actual data in $\mathcal{F}$'s codomain and our parametrized function's predictions, whereas the regularization term (\autoref{eq:regularization}) imposes a restriction on the set $W$ (which is done mainly for numerical stability reasons). The exact definition of $E(W)$ and $R(W)$ is an arbitrary choice. Common used error functions are:
\begin{itemize}
	\item The Mean Square Error (MSE):
	\begin{equation}
		E(W) = \frac{1}{N_{\text{Train}}} \sum_{i=1}^{N_{\text{Train}}} (\mathbf{Y}_i - \mathbf{\hat{Y}}_i)^2
	\end{equation}
	\item The Mean Absolute Error (MAE):
	\begin{equation}
		E(W) = \frac{1}{N_{\text{Train}}} \sum_{i=1}^{N_{\text{Train}}} \left|\mathbf{Y}_i - \mathbf{\hat{Y}}_i\right|
	\end{equation}
\end{itemize}

For the regularization term, a common choice is the $L_2$ norm.\\
%
\indent Let us rewrite now the optimization problem defined in \autoref{eq:Fargmin} the following way:
\begin{equation}
	W^* = argmin\{L(W))\}
\end{equation}

The algorithm commonly used to find $W^*$ is gradient descent:
\begin{equation}
	W(k+1)=W(k)-\eta\nabla_W\mathcal{L}(W)
\end{equation}

Where, again, $\eta$ is an arbitrarily defined scaling factor which receives the name of ''learning rate''.\\
%
\indent A plethora of different algorithms have been tried for ANN training, but the majority of them shares the basic concept with gradient descent. Amongst common techniques, descending learning rates and stochastic gradient descent are perhaps the most widely used. Cfr. \cite{Marsland2015Machine} for further details on training algorithms.

